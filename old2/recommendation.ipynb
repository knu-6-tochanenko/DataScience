{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Обробка сирих даних"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Імпорт необхідних бібліотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import fnmatch\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зчитування CSV файлів датасету"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OList eCommerce Public Data Set\n",
    "olistDir = \"./dataset/\"\n",
    "\n",
    "OL_customers = pd.read_csv(olistDir+\"olist_customers_dataset.csv\")\n",
    "OL_geo = pd.read_csv(olistDir+\"olist_geolocation_dataset.csv\")\n",
    "OL_orders = pd.read_csv(olistDir+\"olist_orders_dataset.csv\")\n",
    "OL_orders_items = pd.read_csv(olistDir+\"olist_order_items_dataset.csv\")\n",
    "OL_orders_payments = pd.read_csv(olistDir+\"olist_order_payments_dataset.csv\")\n",
    "OL_orders_reviews = pd.read_csv(olistDir+\"olist_order_reviews_dataset.csv\")\n",
    "OL_products = pd.read_csv(olistDir+\"olist_products_dataset.csv\")\n",
    "OL_sellers = pd.read_csv(olistDir+\"olist_sellers_dataset.csv\")\n",
    "OL_products_translations = pd.read_csv(\n",
    "    olistDir+\"product_category_name_translation.csv\")\n",
    "\n",
    "\n",
    "OL_customers = OL_customers.dropna(thresh=len(OL_customers.columns)-1)\n",
    "OL_geo = OL_geo.dropna(thresh=len(OL_geo.columns)-1)\n",
    "OL_orders = OL_orders.dropna(thresh=len(OL_orders.columns)-1)\n",
    "OL_orders_items = OL_orders_items.dropna(thresh=len(OL_orders_items.columns)-1)\n",
    "OL_orders_payments = OL_orders_payments.dropna(\n",
    "    thresh=len(OL_orders_payments.columns)-1)\n",
    "OL_orders_reviews = OL_orders_reviews.dropna(\n",
    "    thresh=len(OL_orders_reviews.columns)-1)\n",
    "OL_products = OL_products.dropna(thresh=len(OL_products.columns)-1)\n",
    "OL_sellers = OL_sellers.dropna(thresh=len(OL_sellers.columns)-1)\n",
    "OL_products_translations = OL_products_translations.dropna(\n",
    "    thresh=len(OL_products_translations.columns)-1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Об'єднання кастомерів"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropGeo = OL_geo.drop_duplicates(\"geolocation_zip_code_prefix\")\n",
    "dropGeo = dropGeo[['geolocation_zip_code_prefix', 'geolocation_lat', 'geolocation_lng']]\n",
    "\n",
    "mergedOLCustomers = OL_customers.merge(dropGeo, how='left',left_on=\"customer_zip_code_prefix\", right_on=\"geolocation_zip_code_prefix\")\n",
    "mergedOLCustomers = mergedOLCustomers.rename(columns={'geolocation_lat':'customer_latitude','geolocation_lng':'customer_longitude'})\n",
    "mergedOLCustomers = mergedOLCustomers.drop(\"geolocation_zip_code_prefix\",axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Об'єднання айтемів"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "productsSub = OL_products[['product_id', 'product_category_name', 'product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm']]\n",
    "productsSub.insert(len(productsSub.columns),'product_volume_cm3', OL_products['product_length_cm']*OL_products['product_height_cm']*OL_products['product_width_cm'])\n",
    "productsSub.insert(len(productsSub.columns),'product_density_g_per_cm3',OL_products['product_weight_g']/productsSub['product_volume_cm3'])\n",
    "\n",
    "mergedOLProducts = productsSub.merge(OL_products_translations, how='left', on=\"product_category_name\")\n",
    "mergedOLProduct = mergedOLProducts.drop(\"product_category_name\",axis=1)\n",
    "mergedOLItems = OL_orders_items.merge(mergedOLProducts, how='left', on=\"product_id\")\n",
    "mergedOLItems = mergedOLItems.drop(\"product_category_name\",axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Об'єднання продавців"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedSellers = OL_sellers.merge(dropGeo, how='left', left_on=\"seller_zip_code_prefix\", right_on=\"geolocation_zip_code_prefix\")\n",
    "mergedSellers = mergedSellers.drop(\"geolocation_zip_code_prefix\",axis=1)\n",
    "mergedSellers = mergedSellers.rename(columns={'geolocation_lat':'seller_latitude','geolocation_lng':'seller_longitude'})\n",
    "mergedOLItems = mergedOLItems.merge(mergedSellers, how='left', on=\"seller_id\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Об'єднання всіх даних"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ReviewsSub = OL_orders_reviews[['review_id', 'order_id', 'review_score', 'review_creation_date', 'review_answer_timestamp']]\n",
    "\n",
    "OL_Data = OL_orders.merge(OL_orders_payments, how='left', on='order_id')\n",
    "OL_Data = OL_Data.merge(ReviewsSub, how='left', on='order_id')\n",
    "OL_Data = OL_Data.merge(mergedOLItems, how='left', on='order_id')\n",
    "OL_Data = OL_Data.merge(mergedOLCustomers, how='left', on='customer_id')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Додаткові методи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def PrintDataFrameSummary(frame):\n",
    "    metaData = list()\n",
    "    metaData.append(list([\"Name:\",\"DType:\",\"Non-Null:\",\"Unique:\"]))\n",
    "    for col in list(frame.columns):\n",
    "        metaData.append( list([\"{0}\".format(col), \n",
    "                              \"{0}\".format(frame[col].dtype),\n",
    "                              \"{0}\".format(frame[col].notna().sum()), \n",
    "                              \"{0}\".format(len(set(frame[frame[col].notna()][col])))]))\n",
    "    \n",
    "    formatString = \"\"\n",
    "    \n",
    "    for i in range(len(metaData[0])):\n",
    "        maxLength = len(max([l[i] for l in metaData], key=len))\n",
    "        formatString += \"{\"+\":<{0}\".format(maxLength+8) +\"}\"\n",
    "        \n",
    "    for i in range(len(metaData)):\n",
    "        \n",
    "        print(formatString.format(*metaData[i]))\n",
    "    \n",
    "    \n",
    "        \n",
    "    print()\n",
    "    print(\"Initial Length {0}\".format(len(frame)))\n",
    "    print(\"Dropped NA Length {0}\".format(len(frame.dropna())))\n",
    "    return frame.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перевірка даних"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:                                DType:         Non-Null:        Unique:        \n",
      "order_id                             object         116554           97659          \n",
      "customer_id                          object         116554           97659          \n",
      "order_status                         object         116554           3              \n",
      "order_purchase_timestamp             object         116554           97115          \n",
      "order_approved_at                    object         116539           89287          \n",
      "order_delivered_carrier_date         object         116553           81018          \n",
      "order_delivered_customer_date        object         115228           95664          \n",
      "order_estimated_delivery_date        object         116554           446            \n",
      "payment_sequential                   float64        116551           29             \n",
      "payment_type                         object         116551           4              \n",
      "payment_installments                 float64        116551           24             \n",
      "payment_value                        float64        116551           28707          \n",
      "review_id                            object         50801            41223          \n",
      "review_score                         float64        50801            5              \n",
      "review_creation_date                 object         50801            613            \n",
      "review_answer_timestamp              object         50801            41198          \n",
      "order_item_id                        float64        116553           21             \n",
      "product_id                           object         116553           32440          \n",
      "seller_id                            object         116553           2977           \n",
      "shipping_limit_date                  object         116553           92382          \n",
      "price                                float64        116553           5888           \n",
      "freight_value                        float64        116553           6960           \n",
      "product_weight_g                     float64        114887           2177           \n",
      "product_length_cm                    float64        114887           99             \n",
      "product_height_cm                    float64        114887           102            \n",
      "product_width_cm                     float64        114887           95             \n",
      "product_volume_cm3                   float64        114887           4443           \n",
      "product_density_g_per_cm3            float64        114887           13348          \n",
      "product_category_name_english        object         114864           71             \n",
      "seller_zip_code_prefix               float64        116553           2174           \n",
      "seller_city                          object         116553           597            \n",
      "seller_state                         object         116553           22             \n",
      "seller_latitude                      float64        116296           2167           \n",
      "seller_longitude                     float64        116296           2167           \n",
      "customer_unique_id                   object         116554           94467          \n",
      "customer_zip_code_prefix             int64          116554           14944          \n",
      "customer_city                        object         116554           4098           \n",
      "customer_state                       object         116554           27             \n",
      "customer_latitude                    float64        116245           14785          \n",
      "customer_longitude                   float64        116245           14786          \n",
      "\n",
      "Initial Length 116554\n",
      "Dropped NA Length 49040\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_status</th>\n",
       "      <th>order_purchase_timestamp</th>\n",
       "      <th>order_approved_at</th>\n",
       "      <th>order_delivered_carrier_date</th>\n",
       "      <th>order_delivered_customer_date</th>\n",
       "      <th>order_estimated_delivery_date</th>\n",
       "      <th>payment_sequential</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>...</th>\n",
       "      <th>seller_city</th>\n",
       "      <th>seller_state</th>\n",
       "      <th>seller_latitude</th>\n",
       "      <th>seller_longitude</th>\n",
       "      <th>customer_unique_id</th>\n",
       "      <th>customer_zip_code_prefix</th>\n",
       "      <th>customer_city</th>\n",
       "      <th>customer_state</th>\n",
       "      <th>customer_latitude</th>\n",
       "      <th>customer_longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e481f51cbdc54678b7cc49136f2d6af7</td>\n",
       "      <td>9ef432eb6251297304e76186b10a928d</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-10-02 10:56:33</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>2017-10-04 19:55:00</td>\n",
       "      <td>2017-10-10 21:25:13</td>\n",
       "      <td>2017-10-18 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>...</td>\n",
       "      <td>maua</td>\n",
       "      <td>SP</td>\n",
       "      <td>-23.680114</td>\n",
       "      <td>-46.452454</td>\n",
       "      <td>7c396fd4830fd04220f754e42b4e5bff</td>\n",
       "      <td>3149</td>\n",
       "      <td>sao paulo</td>\n",
       "      <td>SP</td>\n",
       "      <td>-23.574809</td>\n",
       "      <td>-46.587471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e481f51cbdc54678b7cc49136f2d6af7</td>\n",
       "      <td>9ef432eb6251297304e76186b10a928d</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-10-02 10:56:33</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>2017-10-04 19:55:00</td>\n",
       "      <td>2017-10-10 21:25:13</td>\n",
       "      <td>2017-10-18 00:00:00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>voucher</td>\n",
       "      <td>...</td>\n",
       "      <td>maua</td>\n",
       "      <td>SP</td>\n",
       "      <td>-23.680114</td>\n",
       "      <td>-46.452454</td>\n",
       "      <td>7c396fd4830fd04220f754e42b4e5bff</td>\n",
       "      <td>3149</td>\n",
       "      <td>sao paulo</td>\n",
       "      <td>SP</td>\n",
       "      <td>-23.574809</td>\n",
       "      <td>-46.587471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e481f51cbdc54678b7cc49136f2d6af7</td>\n",
       "      <td>9ef432eb6251297304e76186b10a928d</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-10-02 10:56:33</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>2017-10-04 19:55:00</td>\n",
       "      <td>2017-10-10 21:25:13</td>\n",
       "      <td>2017-10-18 00:00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>voucher</td>\n",
       "      <td>...</td>\n",
       "      <td>maua</td>\n",
       "      <td>SP</td>\n",
       "      <td>-23.680114</td>\n",
       "      <td>-46.452454</td>\n",
       "      <td>7c396fd4830fd04220f754e42b4e5bff</td>\n",
       "      <td>3149</td>\n",
       "      <td>sao paulo</td>\n",
       "      <td>SP</td>\n",
       "      <td>-23.574809</td>\n",
       "      <td>-46.587471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53cdb2fc8bc7dce0b6741e2150273451</td>\n",
       "      <td>b0830fb4747a6c6d20dea0b8c802d7ef</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-07-24 20:41:37</td>\n",
       "      <td>2018-07-26 03:24:27</td>\n",
       "      <td>2018-07-26 14:31:00</td>\n",
       "      <td>2018-08-07 15:27:45</td>\n",
       "      <td>2018-08-13 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>boleto</td>\n",
       "      <td>...</td>\n",
       "      <td>belo horizonte</td>\n",
       "      <td>SP</td>\n",
       "      <td>-19.810119</td>\n",
       "      <td>-43.984727</td>\n",
       "      <td>af07308b275d755c9edb36a90c618231</td>\n",
       "      <td>47813</td>\n",
       "      <td>barreiras</td>\n",
       "      <td>BA</td>\n",
       "      <td>-12.169860</td>\n",
       "      <td>-44.988369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47770eb9100c2d0c44946d9cf07ec65d</td>\n",
       "      <td>41ce2a54c0b03bf3443c3d931a367089</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-08-08 08:38:49</td>\n",
       "      <td>2018-08-08 08:55:23</td>\n",
       "      <td>2018-08-08 13:50:00</td>\n",
       "      <td>2018-08-17 18:06:29</td>\n",
       "      <td>2018-09-04 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>...</td>\n",
       "      <td>guariba</td>\n",
       "      <td>SP</td>\n",
       "      <td>-21.362358</td>\n",
       "      <td>-48.232976</td>\n",
       "      <td>3a653a41f6f9fc3d2a113cf8398680e8</td>\n",
       "      <td>75265</td>\n",
       "      <td>vianopolis</td>\n",
       "      <td>GO</td>\n",
       "      <td>-16.746337</td>\n",
       "      <td>-48.514624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           order_id                       customer_id  \\\n",
       "0  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
       "1  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
       "2  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
       "3  53cdb2fc8bc7dce0b6741e2150273451  b0830fb4747a6c6d20dea0b8c802d7ef   \n",
       "4  47770eb9100c2d0c44946d9cf07ec65d  41ce2a54c0b03bf3443c3d931a367089   \n",
       "\n",
       "  order_status order_purchase_timestamp    order_approved_at  \\\n",
       "0    delivered      2017-10-02 10:56:33  2017-10-02 11:07:15   \n",
       "1    delivered      2017-10-02 10:56:33  2017-10-02 11:07:15   \n",
       "2    delivered      2017-10-02 10:56:33  2017-10-02 11:07:15   \n",
       "3    delivered      2018-07-24 20:41:37  2018-07-26 03:24:27   \n",
       "4    delivered      2018-08-08 08:38:49  2018-08-08 08:55:23   \n",
       "\n",
       "  order_delivered_carrier_date order_delivered_customer_date  \\\n",
       "0          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
       "1          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
       "2          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
       "3          2018-07-26 14:31:00           2018-08-07 15:27:45   \n",
       "4          2018-08-08 13:50:00           2018-08-17 18:06:29   \n",
       "\n",
       "  order_estimated_delivery_date  payment_sequential payment_type  ...  \\\n",
       "0           2017-10-18 00:00:00                 1.0  credit_card  ...   \n",
       "1           2017-10-18 00:00:00                 3.0      voucher  ...   \n",
       "2           2017-10-18 00:00:00                 2.0      voucher  ...   \n",
       "3           2018-08-13 00:00:00                 1.0       boleto  ...   \n",
       "4           2018-09-04 00:00:00                 1.0  credit_card  ...   \n",
       "\n",
       "      seller_city  seller_state seller_latitude  seller_longitude  \\\n",
       "0            maua            SP      -23.680114        -46.452454   \n",
       "1            maua            SP      -23.680114        -46.452454   \n",
       "2            maua            SP      -23.680114        -46.452454   \n",
       "3  belo horizonte            SP      -19.810119        -43.984727   \n",
       "4         guariba            SP      -21.362358        -48.232976   \n",
       "\n",
       "                 customer_unique_id customer_zip_code_prefix  customer_city  \\\n",
       "0  7c396fd4830fd04220f754e42b4e5bff                     3149      sao paulo   \n",
       "1  7c396fd4830fd04220f754e42b4e5bff                     3149      sao paulo   \n",
       "2  7c396fd4830fd04220f754e42b4e5bff                     3149      sao paulo   \n",
       "3  af07308b275d755c9edb36a90c618231                    47813      barreiras   \n",
       "4  3a653a41f6f9fc3d2a113cf8398680e8                    75265     vianopolis   \n",
       "\n",
       "  customer_state customer_latitude customer_longitude  \n",
       "0             SP        -23.574809         -46.587471  \n",
       "1             SP        -23.574809         -46.587471  \n",
       "2             SP        -23.574809         -46.587471  \n",
       "3             BA        -12.169860         -44.988369  \n",
       "4             GO        -16.746337         -48.514624  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PrintDataFrameSummary(OL_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "order_id                          object\n",
       "customer_id                       object\n",
       "order_status                      object\n",
       "order_purchase_timestamp          object\n",
       "order_approved_at                 object\n",
       "order_delivered_carrier_date      object\n",
       "order_delivered_customer_date     object\n",
       "order_estimated_delivery_date     object\n",
       "payment_sequential               float64\n",
       "payment_type                      object\n",
       "payment_installments             float64\n",
       "payment_value                    float64\n",
       "review_id                         object\n",
       "review_score                     float64\n",
       "review_creation_date              object\n",
       "review_answer_timestamp           object\n",
       "order_item_id                    float64\n",
       "product_id                        object\n",
       "seller_id                         object\n",
       "shipping_limit_date               object\n",
       "price                            float64\n",
       "freight_value                    float64\n",
       "product_weight_g                 float64\n",
       "product_length_cm                float64\n",
       "product_height_cm                float64\n",
       "product_width_cm                 float64\n",
       "product_volume_cm3               float64\n",
       "product_density_g_per_cm3        float64\n",
       "product_category_name_english     object\n",
       "seller_zip_code_prefix           float64\n",
       "seller_city                       object\n",
       "seller_state                      object\n",
       "seller_latitude                  float64\n",
       "seller_longitude                 float64\n",
       "customer_unique_id                object\n",
       "customer_zip_code_prefix           int64\n",
       "customer_city                     object\n",
       "customer_state                    object\n",
       "customer_latitude                float64\n",
       "customer_longitude               float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OL_Data.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конвертація дати"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dateCol in ['order_purchase_timestamp',\n",
    "                'order_approved_at',\n",
    "                'order_delivered_carrier_date',\n",
    "                'order_delivered_customer_date', \n",
    "                'order_estimated_delivery_date',\n",
    "                'review_creation_date',\n",
    "                'review_answer_timestamp',\n",
    "                'shipping_limit_date'\n",
    "               ]:\n",
    "    OL_Data[dateCol] = pd.to_datetime(OL_Data[dateCol])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Статуси замовлень"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "delivered    115228\n",
       "shipped        1246\n",
       "canceled         80\n",
       "Name: order_status, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OL_Data['order_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "credit_card    86005\n",
       "boleto         22629\n",
       "voucher         6237\n",
       "debit_card      1680\n",
       "Name: payment_type, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OL_Data['payment_type'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генерація одного датасету"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vlad\\AppData\\Local\\Temp\\ipykernel_18812\\1632512777.py:4: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  itemGroup = itemSub.groupby('order_id', as_index=False).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itemGroup\n",
      "                           order_id  Total_price  number_of_items\n",
      "0  00010242fe8c5a6d1ba2dd792cb16214        58.90                1\n",
      "1  00018f77f2f0320c557190d7a144bdd3       239.90                1\n",
      "2  000229ec398224ef6ca0657da4fc703e       199.00                1\n",
      "3  00024acbcdf0a6daa1e931b038114c75        12.99                1\n",
      "4  00042b26cf59d7ce69dfabb4e55b4fd9       199.90                1\n",
      "\n",
      "paymentsMed\n",
      "                           order_id  Median_payments\n",
      "0  00010242fe8c5a6d1ba2dd792cb16214            72.19\n",
      "1  00018f77f2f0320c557190d7a144bdd3           259.83\n",
      "2  000229ec398224ef6ca0657da4fc703e           216.87\n",
      "3  00024acbcdf0a6daa1e931b038114c75            25.78\n",
      "4  00042b26cf59d7ce69dfabb4e55b4fd9           218.04\n",
      "\n",
      "paymentsTot\n",
      "                           order_id  Total_payment  Total_Freight\n",
      "0  00010242fe8c5a6d1ba2dd792cb16214          72.19          13.29\n",
      "1  00018f77f2f0320c557190d7a144bdd3         259.83          19.93\n",
      "2  000229ec398224ef6ca0657da4fc703e         216.87          17.87\n",
      "3  00024acbcdf0a6daa1e931b038114c75          25.78          12.79\n",
      "4  00042b26cf59d7ce69dfabb4e55b4fd9         218.04          18.14\n",
      "\n",
      "ordersSize\n",
      "                           order_id  order_weight_g  order_volume_cm3\n",
      "0  00010242fe8c5a6d1ba2dd792cb16214           650.0            3528.0\n",
      "1  00018f77f2f0320c557190d7a144bdd3         30000.0           60000.0\n",
      "2  000229ec398224ef6ca0657da4fc703e          3050.0           14157.0\n",
      "3  00024acbcdf0a6daa1e931b038114c75           200.0            2400.0\n",
      "4  00042b26cf59d7ce69dfabb4e55b4fd9          3750.0           42000.0\n",
      "\n",
      "paymentsType\n",
      "                           order_id payment_type\n",
      "0  00010242fe8c5a6d1ba2dd792cb16214  credit_card\n",
      "1  00018f77f2f0320c557190d7a144bdd3  credit_card\n",
      "2  000229ec398224ef6ca0657da4fc703e  credit_card\n",
      "3  00024acbcdf0a6daa1e931b038114c75  credit_card\n",
      "4  00042b26cf59d7ce69dfabb4e55b4fd9  credit_card\n"
     ]
    }
   ],
   "source": [
    "#Total Price and Total Items in Order\n",
    "itemSub = OL_Data[['order_id', 'order_item_id','price','product_id']]\n",
    "itemSub = itemSub.drop_duplicates()\n",
    "itemGroup = itemSub.groupby('order_id', as_index=False).sum()\n",
    "itemGroup['number_of_items'] = itemGroup['order_item_id'].map({0:0,\n",
    "                                                                1:1,\n",
    "                                                                3:2,\n",
    "                                                                6:3,\n",
    "                                                                10:4,\n",
    "                                                                15:5,\n",
    "                                                                21:6,\n",
    "                                                                28:7,\n",
    "                                                                36:8,\n",
    "                                                                45:9,\n",
    "                                                                55:10,\n",
    "                                                                66:11,\n",
    "                                                                78:12,\n",
    "                                                                91:13,\n",
    "                                                                105:14,\n",
    "                                                                120:15,\n",
    "                                                                210:20,\n",
    "                                                                231:21})\n",
    "itemGroup = itemGroup.drop(['order_item_id'],axis=1)\n",
    "itemGroup = itemGroup.rename(columns={\"price\":\"Total_price\"})\n",
    "print(\"itemGroup\")\n",
    "print(itemGroup.head())\n",
    "\n",
    "\n",
    "#Median Payments, useful for installment plans\n",
    "paymentSub = OL_Data[['order_id',\n",
    "                      'payment_value',\n",
    "                      'payment_type',\n",
    "                      'freight_value',\n",
    "                     'payment_sequential']]\n",
    "paymentSub = paymentSub.drop_duplicates()\n",
    "payments = paymentSub[['order_id','payment_value','freight_value']]\n",
    "paymentsMed = payments[['order_id','payment_value']].groupby('order_id', as_index=False).median()\n",
    "paymentsMed = paymentsMed.rename(columns ={\"payment_value\":\"Median_payments\"})\n",
    "print()\n",
    "print(\"paymentsMed\")\n",
    "print(paymentsMed.head())\n",
    "\n",
    "#Total Payments, Total Frieght Cost Value\n",
    "paymentsTot = payments.groupby('order_id', as_index=False).sum()\n",
    "paymentsTot = paymentsTot.rename(columns = {\"payment_value\":\"Total_payment\",\"freight_value\":\"Total_Freight\"})\n",
    "print()\n",
    "print(\"paymentsTot\")\n",
    "print(paymentsTot.head())\n",
    "\n",
    "#Total Weight and Volume of Order\n",
    "ordersSize = OL_Data[['order_id', \n",
    "                        'order_item_id',\n",
    "                        'price',\n",
    "                        'product_id',\n",
    "                        'product_weight_g',\n",
    "                        'product_volume_cm3']]\n",
    "ordersSize = ordersSize.drop_duplicates()\n",
    "ordersSize = ordersSize[['order_id',\n",
    "                         'product_weight_g',\n",
    "                        'product_volume_cm3']].groupby('order_id', as_index=False).sum()\n",
    "ordersSize = ordersSize.rename(columns = {'product_weight_g':'order_weight_g',\n",
    "                                         'product_volume_cm3':'order_volume_cm3'})\n",
    "print()\n",
    "print(\"ordersSize\")\n",
    "print(ordersSize.head())\n",
    "\n",
    "#Payment Type\n",
    "paymentsType = paymentSub[['order_id','payment_sequential','payment_type']]\n",
    "paymentsType = paymentsType.sort_values(by='payment_sequential')[['order_id', 'payment_type']]\n",
    "paymentsType = paymentsType.groupby('order_id', as_index=False).first()\n",
    "print()\n",
    "print(\"paymentsType\")\n",
    "print(paymentsType.head())\n",
    "\n",
    "\n",
    "#Merge to Single DataFrame.\n",
    "#An order may still have multiple sellers\n",
    "NonGroupedData = OL_Data[['order_id',  \n",
    "                          'order_status', \n",
    "                          'order_purchase_timestamp',\n",
    "                          'order_approved_at', \n",
    "                          'order_delivered_carrier_date',\n",
    "                          'order_delivered_customer_date', \n",
    "                          'order_estimated_delivery_date',\n",
    "                          'shipping_limit_date',\n",
    "                          'payment_installments',\n",
    "                          'product_category_name_english', \n",
    "                          'seller_id', \n",
    "                          'seller_zip_code_prefix',\n",
    "                          'seller_city', \n",
    "                          'seller_state', \n",
    "                          'seller_latitude', \n",
    "                          'seller_longitude',\n",
    "                          'customer_id',\n",
    "                          'customer_unique_id', \n",
    "                          'customer_zip_code_prefix', \n",
    "                          'customer_city',\n",
    "                          'customer_state', \n",
    "                          'customer_latitude', \n",
    "                          'customer_longitude',\n",
    "                          'review_id', \n",
    "                          'review_score', \n",
    "                          'review_creation_date',\n",
    "                          'review_answer_timestamp']]\n",
    "NonGroupedData= NonGroupedData.drop_duplicates()\n",
    "OrderData = NonGroupedData.merge(itemGroup,on='order_id')\n",
    "OrderData = OrderData.merge(paymentsMed, on='order_id')\n",
    "OrderData = OrderData.merge(paymentsTot, on='order_id')\n",
    "OrderData = OrderData.merge(ordersSize, on='order_id')\n",
    "OrderData = OrderData.merge(paymentsType, on='order_id')\n",
    "\n",
    "\n",
    "#Generate Delivery Features\n",
    "OrderData['Purchase_To_Delivery_Days'] = [x.days for x in OrderData['order_delivered_customer_date']-OrderData['order_purchase_timestamp']]\n",
    "OrderData['Approved_To_Delivery_Days'] = [x.days for x in OrderData['order_delivered_customer_date']-OrderData['order_approved_at']]\n",
    "OrderData['Diff_Est_Delivery_vs_Actual'] = [x.days for x in OrderData['order_delivered_customer_date']-OrderData['order_estimated_delivery_date']]\n",
    "\n",
    "OrderData.to_csv(\"./dataset/OL_Data_Order_Grouped.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Recomendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmath\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpairwise\u001b[39;00m \u001b[39mimport\u001b[39;00m cosine_similarity\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from IPython.display import clear_output\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getRecommendataionsFromCategoryGroupDict\n",
    "#  Take a list of orders, a unique customer id in the orders and a Category DataFrame (columns represent \n",
    "#  categories of the Orders, and each row is a Group that sums to 100% with this percentage spread out \n",
    "#  among the columns and every row is unique) and return a dictionary of product ids as keys and the cosine\n",
    "#  simularity between this users orders and a row of the Category DataFrame.  \n",
    "#\n",
    "#  Only products that the customer has not bought before will appear in the returned dictionary, and products\n",
    "#  that are repeated will be listed once with its highest simularity score.  In addition, the user passes in\n",
    "#  a minimum simularity score, and any Group whose simularity with this user is below this threshold will not\n",
    "#  appear in the returned recommendations\n",
    "#\n",
    "#  orders - a list of orders from the OL_Data dataset, either the whole thing or simplified columns\n",
    "#  custUniqueId - the ID of the customer to create recommendations for\n",
    "#  productColName - name of the column that contains the unique product identifier\n",
    "#  customerColName - name of the column that contains the unique customer identifier\n",
    "#  categoryGroupsDF - Category DataFrame as described above, columns are categories, rows are Groups\n",
    "#  categoryGroupsDF_DropCols - Columns in categoryGroupsDF to ignore\n",
    "#  categoryGroupsDF_CatColumnName - the column name in the orders that contains the categories in the columns\n",
    "#                                   of categoryGroupsDF\n",
    "#  categoryGroupsDF_groupIdName - the column name of the unique Group (row) identifier in categoryGroupsDF\n",
    "#  minimumSimularity - value between 0 and 1, any simularity lower than this will be ignored\n",
    "#  includeRFM (optional) - Boolean to include/exclude the RFM score in recommendation\n",
    "#  RFM_ColName (optional) - Column for RFM Score, only used if includeRFM is True\n",
    "def getRecommendataionsFromCategoryGroupDict(orders,\n",
    "                                             custUniqueId,\n",
    "                                             \n",
    "                                             productColName,\n",
    "                                             customerColName,\n",
    "                                             \n",
    "                                             categoryGroupsDF,\n",
    "                                             categoryGroupsDF_DropCols,\n",
    "                                             categoryGroupsDF_CatColumnName,\n",
    "                                             categoryGroupsDF_groupIdName,\n",
    "                                             \n",
    "                                             minimumSimularity,\n",
    "                                             \n",
    "                                             includeRFM = False,\n",
    "                                             RFM_ColName = None):\n",
    "\n",
    "    #Remove ignored columns from CategoryGroupsDF, and set index\n",
    "    catGroupsDF_Indexed = categoryGroupsDF.drop(columns=categoryGroupsDF_DropCols).set_index(categoryGroupsDF_groupIdName)\n",
    "    \n",
    "    #Get all categories in the Group\n",
    "    all_categories = list(catGroupsDF_Indexed.columns)\n",
    "    all_categories = [str(x) for x in all_categories]    \n",
    "    \n",
    "    #Set up the Product Recommendation Dict, empty dict returned if no user orders, or no simularity high enough\n",
    "    RecommendationDict = {}    \n",
    "    RecommendationDict['IdType']=categoryGroupsDF_groupIdName+\"_CosSim\"\n",
    "    \n",
    "    #Create the Row for the customer to use for cosine simularity\n",
    "    thiscustSubset = orders[orders[customerColName] == custUniqueId]\n",
    "    thisCustCategories = np.array([len(thiscustSubset[thiscustSubset[categoryGroupsDF_CatColumnName] == x]) for x in all_categories])\n",
    "    \n",
    "    #No user orders\n",
    "    if(sum(thisCustCategories) == 0):\n",
    "        return RecommendationDict\n",
    "    \n",
    "    #Normalize customer's row to percentages in categories, and create DF\n",
    "    thisCustCategories = 100*thisCustCategories/sum(thisCustCategories)\n",
    "    thisCustCategories.shape = (1, len(thisCustCategories))\n",
    "    custRowDF = pd.DataFrame(data=thisCustCategories, columns=all_categories)    \n",
    "\n",
    "    #Calculate simularity scores, and add to dataframe for sorting against Group ID\n",
    "    simularity = cosine_similarity(catGroupsDF_Indexed, custRowDF)\n",
    "    simDF = pd.DataFrame(simularity, columns=['Simularity'])\n",
    "    simDF[categoryGroupsDF_groupIdName] = catGroupsDF_Indexed.reset_index()[categoryGroupsDF_groupIdName]\n",
    "    \n",
    "    #Get orders with product_ids this user has not bought\n",
    "    thisCustomerProducts = thiscustSubset[productColName].unique()\n",
    "    newProductOrders = OL_Data_Simp[~OL_Data_Simp[productColName].isin(thisCustomerProducts)]    \n",
    "    \n",
    "    #Filter out Group IDs that are not simular enough\n",
    "    SimilarEnoughDF = simDF[simDF.Simularity>=minimumSimularity].sort_values(by='Simularity', ascending=False) \n",
    "    for index, row in SimilarEnoughDF.iterrows():\n",
    "        \n",
    "        prob = row['Simularity']\n",
    "            \n",
    "        if includeRFM and RFM_ColName != None:\n",
    "            maxRFM = max(orders[RFM_ColName])\n",
    "            \n",
    "            #Get Product Ids, keeping the id with the highest specified RFM\n",
    "            thisProdRecs = newProductOrders[newProductOrders[categoryGroupsDF_groupIdName]==row[categoryGroupsDF_groupIdName]][[productColName, RFM_ColName]]\n",
    "            thisProdRecs = thisProdRecs.sort_values(by=RFM_ColName)\n",
    "            thisProdRecs = thisProdRecs.drop_duplicates(subset=[productColName])\n",
    "            \n",
    "            #Go through collected Product Ids, replacing existing if new score is higher\n",
    "            #or adding new\n",
    "            for i,row in thisProdRecs.iterrows():\n",
    "                adjustedProb = prob+row[RFM_ColName]/maxRFM\n",
    "                \n",
    "                if(row[productColName] in RecommendationDict.keys()):\n",
    "                    RecommendationDict[row[productColName]] = max(RecommendationDict[row[productColName]], adjustedProb)\n",
    "                else:\n",
    "                    RecommendationDict[row[productColName]] = adjustedProb\n",
    "        else:\n",
    "            #Get product_ids of this Group, and add those not already in the Recommendations\n",
    "            thisProdRecs = newProductOrders[newProductOrders[categoryGroupsDF_groupIdName] == row[categoryGroupsDF_groupIdName]][productColName].unique()\n",
    "\n",
    "            for prod in thisProdRecs:\n",
    "                if(prod in RecommendationDict.keys()):\n",
    "                    continue\n",
    "                \n",
    "                RecommendationDict[prod] = prob\n",
    "    #Return the product recommendations\n",
    "    return RecommendationDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getRecommendataionsFromSimilarityMatrix\n",
    "#  Take a list of orders, a unique customer id in the orders, and a similarity matrix with similarity scores\n",
    "#  between groups.  Also provided are the name of the Group category, which needs to be in the list of orders\n",
    "#  and the user given needs to be assigned to one.  Using the similarity matrix, recommendations will be given \n",
    "#  from Groups in order of their similarity to the customer\n",
    "#\n",
    "#  Only products that the customer has not bought before will appear in the returned dictionary, and products\n",
    "#  that are repeated will be listed once with its highest simularity score.  In addition, the user passes in\n",
    "#  a minimum simularity score, and any Group whose simularity with this user is below this threshold will not\n",
    "#  appear in the returned recommendations\n",
    "#\n",
    "#  orders - a list of orders from the OL_Data dataset, either the whole thing or simplified columns\n",
    "#  custUniqueId - the ID of the customer to create recommendations for\n",
    "#  productColName - name of the column that contains the unique product identifier\n",
    "#  customerColName - name of the column that contains the unique customer identifier\n",
    "#  SimilarityMatrix - n x n+1 matrix, with similarity scores of every group to every other, a group ID column\n",
    "#  GroupIdName - the column name of the unique Group identifier in SimilarityMatrix\n",
    "#  minimumSimularity - value between 0 and 1, any simularity lower than this will be ignored\n",
    "#  includeRFM (optional) - Boolean to include/exclude the RFM score in recommendation\n",
    "#  RFM_ColName (optional) - Column for RFM Score, only used if includeRFM is True\n",
    "def getRecommendataionsFromSimilarityMatrix(orders,\n",
    "                                             custUniqueId,\n",
    "                                             productColName,\n",
    "                                             customerColName,\n",
    "                                             \n",
    "                                             SimilarityMatrix,\n",
    "                                             GroupIdName,\n",
    "                                             \n",
    "                                             minimumSimularity,\n",
    "                                            \n",
    "                                             includeRFM = False,\n",
    "                                             RFM_ColName = None):\n",
    "\n",
    "    \n",
    "    GroupId = orders[orders[customerColName]==custUniqueId][GroupIdName].iloc[0]\n",
    "    \n",
    "    #Set up the Product Recommendation Dict, empty dict returned if no user orders, or no simularity high enough\n",
    "    RecommendationDict = {}    \n",
    "    RecommendationDict['IdType']=GroupIdName+\"_CosSim\"\n",
    "    \n",
    "    #Get Similarity Scores for this Group\n",
    "    if(len(SimilarityMatrix[SimilarityMatrix[GroupIdName]==GroupId]) == 0 ):\n",
    "        return RecommendationDict\n",
    "    \n",
    "    #Get the Groups with similarity above threshold\n",
    "    GroupId_Str = str(int(GroupId))\n",
    "    simScores = SimilarityMatrix[[GroupIdName,GroupId_Str]]\n",
    "    simScores = simScores[simScores[GroupId_Str]>= minimumSimularity].sort_values(by=GroupId_Str)\n",
    "    \n",
    "    #Get orders with product_ids this user has not bought\n",
    "    thiscustSubset = orders[orders[customerColName] == custUniqueId]\n",
    "    thisCustomerProducts = thiscustSubset[productColName].unique()\n",
    "    newProductOrders = OL_Data_Simp[~OL_Data_Simp[productColName].isin(thisCustomerProducts)]    \n",
    "    \n",
    "    for index, row in simScores.iterrows():\n",
    "        \n",
    "        prob = row[GroupId_Str]\n",
    "            \n",
    "        if includeRFM and RFM_ColName != None:\n",
    "            maxRFM = max(orders[RFM_ColName])\n",
    "            \n",
    "            #Get Product Ids, keeping the id with the highest specified RFM\n",
    "            thisProdRecs = newProductOrders[newProductOrders[GroupIdName]==row[GroupIdName]][[productColName, RFM_ColName]]\n",
    "            thisProdRecs = thisProdRecs.sort_values(by=RFM_ColName)\n",
    "            thisProdRecs = thisProdRecs.drop_duplicates(subset=[productColName])\n",
    "            \n",
    "            #Go through collected Product Ids, replacing existing if new score is higher\n",
    "            #or adding new\n",
    "            for i,row in thisProdRecs.iterrows():\n",
    "                adjustedProb = prob+row[RFM_ColName]/maxRFM\n",
    "                \n",
    "                if(row[productColName] in RecommendationDict.keys()):\n",
    "                    RecommendationDict[row[productColName]] = max(RecommendationDict[row[productColName]], adjustedProb)\n",
    "                else:\n",
    "                    RecommendationDict[row[productColName]] = adjustedProb\n",
    "        else:\n",
    "            #Get product_ids of this Group, and add those not already in the Recommendations\n",
    "            thisProdRecs = newProductOrders[newProductOrders[GroupIdName] == row[GroupIdName]][productColName].unique()\n",
    "\n",
    "            for prod in thisProdRecs:\n",
    "                if(prod in RecommendationDict.keys()):\n",
    "                    continue\n",
    "                \n",
    "                RecommendationDict[prod] = prob\n",
    "    #Return the product recommendations\n",
    "    return RecommendationDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from IPython.display import clear_output\n",
    "import copy\n",
    "\n",
    "\n",
    "#getRecommendataionsFromAglomCluster\n",
    "#  Take a list of orders, a unique customer id in the orders and a Category DataFrame (columns represent \n",
    "#  categories of the Orders, and each row is a Group that sums to 100% with this percentage spread out \n",
    "#  among the columns and every row is unique) and return a dictionary of product ids as keys and the silouette\n",
    "#  score of the Groups in the same cluster as the customer\n",
    "#\n",
    "#  Only products that the customer has not bought before will appear in the returned dictionary, and products\n",
    "#  that are repeated will be listed once with its highest silouette score.  In addition, the user passes in\n",
    "#  a minimum simularity score, and any Group whose simularity with this user is below this threshold will not\n",
    "#  appear in the returned recommendations\n",
    "#\n",
    "#  orders - a list of orders from the OL_Data dataset, either the whole thing or simplified columns\n",
    "#  custUniqueId - the ID of the customer to create recommendations for\n",
    "#  productColName - name of the column that contains the unique product identifier\n",
    "#  customerColName - name of the column that contains the unique customer identifier\n",
    "#  categoryGroupsDF - Category DataFrame as described above, columns are categories, rows are Groups\n",
    "#  categoryGroupsDF_DropCols - Columns in categoryGroupsDF to ignore\n",
    "#  categoryGroupsDF_CatColumnName - the column name in the orders that contains the categories in the columns\n",
    "#                                   of categoryGroupsDF\n",
    "#  categoryGroupsDF_groupIdName - the column name of the unique Group (row) identifier in categoryGroupsDF\n",
    "#  minimumSimularity - value between 0 and 1, any simularity lower than this will be ignored\n",
    "#  includeRFM (optional) - Boolean to include/exclude the RFM score in recommendation\n",
    "#  RFM_ColName (optional) - Column for RFM Score, only used if includeRFM is True\n",
    "def getRecommendataionsFromAglomCluster(orders,\n",
    "                                        custUniqueId,\n",
    "                                        \n",
    "                                         productColName,\n",
    "                                         customerColName,\n",
    "\n",
    "                                         categoryGroupsDF,\n",
    "                                         categoryGroupsDF_DropCols,\n",
    "                                         categoryGroupsDF_CatColumnName,\n",
    "                                         categoryGroupsDF_groupIdName,\n",
    "\n",
    "                                         minimumSimularity,\n",
    "                                         includeRFM = False,\n",
    "                                         RFM_ColName = None):\n",
    "    \n",
    "    #Remove ignored columns from CategoryGroupsDF, and set index\n",
    "    catGroupsDF_Indexed = categoryGroupsDF.drop(columns=categoryGroupsDF_DropCols).set_index(categoryGroupsDF_groupIdName).sort_values(by=categoryGroupsDF_groupIdName)\n",
    "    \n",
    "    #Get all categories in the Group\n",
    "    all_categories = list(catGroupsDF_Indexed.columns)\n",
    "    all_categories = [str(x) for x in all_categories]    \n",
    "    \n",
    "    #Set up the Product Recommendation Dict, empty dict returned if no user orders, or no simularity high enough\n",
    "    RecommendationDict = {}    \n",
    "    RecommendationDict['IdType']=categoryGroupsDF_groupIdName+\"_Aglom\"\n",
    "    \n",
    "    #Get the Row for the customer to use Agglomerative Clustering\n",
    "    thiscustSubset = orders[orders[customerColName] == custUniqueId]\n",
    "    thisCustCategories = np.array([len(thiscustSubset[thiscustSubset[categoryGroupsDF_CatColumnName] == x]) for x in all_categories])\n",
    "    \n",
    "    #No user orders\n",
    "    if(sum(thisCustCategories) ==0):\n",
    "        return RecommendationDict\n",
    "    \n",
    "    #Normalize customer's row to percentages in categories, and create DF\n",
    "    thisCustCategories = 100*thisCustCategories/sum(thisCustCategories)\n",
    "    thisCustCategories.shape = (1, len(thisCustCategories))\n",
    "    custRowDF = pd.DataFrame(data=thisCustCategories, columns=all_categories)    \n",
    "    \n",
    "\n",
    "    #Add customer to Cat Groups for clustering,\n",
    "    allDF = pd.concat([catGroupsDF_Indexed,custRowDF], ignore_index=True)\n",
    "    model = AgglomerativeClustering(n_clusters=30, linkage=\"average\").fit(allDF)\n",
    "    #Get clusters for customer and others\n",
    "    clusterAssignments = model.labels_\n",
    "    cusCluster = clusterAssignments[-1]\n",
    "    otherClusters = clusterAssignments[:-1]\n",
    "    #Get silouette scores for customer and others\n",
    "    scores = silhouette_samples(allDF,clusterAssignments, metric ='cosine')\n",
    "    cusScore = scores[-1]\n",
    "    otherScores = scores[:-1]\n",
    "    \n",
    "    #Add cluster assignments and silouette to Category DF\n",
    "    catGroupsDF_Indexed['assignedTo'] = otherClusters\n",
    "    catGroupsDF_Indexed['s_score'] = otherScores\n",
    "    catGroupsDF_Indexed = catGroupsDF_Indexed.reset_index()\n",
    "    \n",
    "    #Get groups in same cluster of user and filter out sillouette score lower than threshold\n",
    "    group = catGroupsDF_Indexed[catGroupsDF_Indexed['assignedTo']==cusCluster]\n",
    "    group = group[group['s_score']>=minimumSimularity].sort_values(by='s_score')\n",
    "    \n",
    "    #Get orders with products the user has not bought\n",
    "    thisCustomerProducts = thiscustSubset[productColName].unique()\n",
    "    newProductOrders = OL_Data_Simp[~OL_Data_Simp[productColName].isin(thisCustomerProducts)]\n",
    "    \n",
    "    \n",
    "    for i,row in group.iterrows():\n",
    "        prob = row['s_score']\n",
    "        \n",
    "        if includeRFM and RFM_ColName != None:\n",
    "            maxRFM = max(orders[RFM_ColName])\n",
    "            \n",
    "            #Get Product Ids, keeping the id with the highest specified RFM\n",
    "            thisProdRecs = newProductOrders[newProductOrders[categoryGroupsDF_groupIdName] == row[categoryGroupsDF_groupIdName]][[productColName, RFM_ColName]]\n",
    "            thisProdRecs = thisProdRecs.sort_values(by=RFM_ColName)\n",
    "            thisProdRecs = thisProdRecs.drop_duplicates(subset=[productColName])\n",
    "            \n",
    "            #Go through collected Product Ids, replacing existing if new score is higher\n",
    "            #or adding new\n",
    "            for i,row in thisProdRecs.iterrows():\n",
    "                adjustedProb = prob+row[RFM_ColName]/maxRFM\n",
    "                \n",
    "                if(row[productColName] in RecommendationDict.keys()):\n",
    "                    RecommendationDict[row[productColName]] = max(RecommendationDict[row[productColName]], adjustedProb)\n",
    "                else:\n",
    "                    RecommendationDict[row[productColName]] = adjustedProb\n",
    "        else:\n",
    "            #Get product_ids of this Group, and add those not already in the Recommendations\n",
    "            thisProdRecs = newProductOrders[newProductOrders[categoryGroupsDF_groupIdName] == row[categoryGroupsDF_groupIdName]][productColName].unique()\n",
    "\n",
    "            for prod in thisProdRecs:\n",
    "                if(prod in RecommendationDict.keys()):\n",
    "                    continue\n",
    "                \n",
    "                RecommendationDict[prod] = prob\n",
    "    #return recommendations\n",
    "    return RecommendationDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getRecommendataionsForCustomer\n",
    "#  Take a list of orders, a unique customer id in the orders and a dictionary containing Category Groups of the form:\n",
    "#      key : Unique Group Identifier in Category Group\n",
    "#      value : dict{\n",
    "#                     'Groups': DF of Groups (rows) and Categories (columns),\n",
    "#                     'DropCols': columns to ignore in the DF,\n",
    "#                     'CategoryColName': column name in orders that contains values used as columns for DF\n",
    "#                  }\n",
    "#\n",
    "#  For each Category Group, find recommendations using cosine simularity and Agglomerative clustering that \n",
    "#  are above a minimum threshold provided. \n",
    "#\n",
    "#  Gather the recommendations into a DataFrame and sum them into a column called Total.  Return a DataFrame\n",
    "#  of recommendations for the user sorted by the total score\n",
    "#\n",
    "#  orders - a list of orders from the OL_Data dataset, either the whole thing or simplified columns\n",
    "#  custUniqueId - the ID of the customer to create recommendations for\n",
    "#  CatgoryGroupDict - Dictionary of Category Groups, as discribed above\n",
    "#  minimumSimularity - value between 0 and 1, any simularity lower than this will be ignored\n",
    "#  includeRFM (optional) - Boolean to include/exclude the RFM score in recommendation\n",
    "#  RFM_ColName (optional) - Column for RFM Score, only used if includeRFM is True\n",
    "def getRecommendataionsForCustomer(orders,\n",
    "                                   customerUniqueId,\n",
    "                                    CategoryGroupDict,\n",
    "                                    minimumSimularity,\n",
    "                                    includeRFM = False):\n",
    "    #Array to make DF from\n",
    "    recomendationArray = []\n",
    "    \n",
    "    \n",
    "    for i, (k,v) in enumerate(CategoryGroupDict.items()):\n",
    "        \n",
    "        #Add Cosine Simularity Recommendations to the Array\n",
    "        recomendationArray.append(\n",
    "            getRecommendataionsFromCategoryGroupDict(\n",
    "                                    orders,\n",
    "                                    customerUniqueId,\n",
    "                                    'product_id',\n",
    "                                    'customer_unique_id',\n",
    "                                    v['Groups'],\n",
    "                                    v['DropCols'],\n",
    "                                    v['CategoryColName'],\n",
    "                                    k,\n",
    "                                    minimumSimularity,\n",
    "                                    includeRFM,\n",
    "                                    v['RFMCol'])\n",
    "        )\n",
    "        #Add Agglomerative Simularity Recommendations to the Array\n",
    "        recomendationArray.append(\n",
    "            getRecommendataionsFromAglomCluster(\n",
    "                                    orders,\n",
    "                                    customerUniqueId,\n",
    "                                    'product_id',\n",
    "                                    'customer_unique_id',\n",
    "                                    v['Groups'],\n",
    "                                    v['DropCols'],\n",
    "                                    v['CategoryColName'],\n",
    "                                    k,\n",
    "                                    minimumSimularity,\n",
    "                                    includeRFM,\n",
    "                                    v['RFMCol'])\n",
    "        )\n",
    "    \n",
    "    #Create Product Rec DF from array, and fill NA with 0\n",
    "    recs = pd.DataFrame(recomendationArray)\n",
    "    allRecs = recs.rename(columns={'IdType':'index'}).set_index('index').T.reset_index().rename(columns={'index':'product_id'}).fillna(0)\n",
    "    \n",
    "    #Create Total Column, a sum of all the other recommendation scores\n",
    "    scoreTypes = recs.IdType.unique()\n",
    "    allRecs['Total'] = np.zeros(len(allRecs))\n",
    "    for scoreType in scoreTypes:\n",
    "        allRecs['Total'] = allRecs['Total']+allRecs[scoreType]\n",
    "    \n",
    "    #Return Recommendations\n",
    "    return allRecs.sort_values(by='Total', ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Order Data\n",
    "OL_Data = pd.read_csv(\"./dataset/OL_Data_Cat_Simplified.csv\")\n",
    "\n",
    "OL_Data_RFM = pd.read_csv(\"./OLData_RFM_Metrics.csv\")\n",
    "buyerRFMDF = OL_Data_RFM[['customer_unique_id','Buyer_OverallRFMScore']].drop_duplicates()\n",
    "sellerRFMDF = OL_Data_RFM[['seller_id','Seller_OverallRFMScore']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simplify the Orders Data Frame\n",
    "OL_Data_Simp = OL_Data[['order_id',\n",
    "                        'customer_unique_id',\n",
    "                        'product_id',\n",
    "                        'product_category_name_english',\n",
    "                        'seller_id',\n",
    "                        'SellerGroupId',\n",
    "                        'CustomerGroupId',\n",
    "                        'CustomerProductGroupId']]\n",
    "OL_Data_Simp = OL_Data_Simp.drop_duplicates()\n",
    "OL_Data_Simp = OL_Data_Simp.merge(buyerRFMDF, how='left', on='customer_unique_id') \n",
    "OL_Data_Simp = OL_Data_Simp.merge(sellerRFMDF, how='left', on='seller_id') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Category Groups Dict to Generate Recommendations\n",
    "CategoryGroups = {}\n",
    "\n",
    "#Add Seller-Category Groups\n",
    "SellerCategoryGroups = pd.read_csv(\"./SellerCategoryUniqueGroups.csv\").sort_values(by='SellerGroupId')\n",
    "SellerSimularityScores = pd.read_csv(\"./SellerCategoryCosineSimularity.csv\").sort_values(by='SellerGroupId')\n",
    "SellerCatDict = {'Groups':SellerCategoryGroups, \n",
    "                 'DropCols':['size'],\n",
    "                 'CategoryColName':'product_category_name_english',\n",
    "                 'CatIdentifier':'seller_id',\n",
    "                 'Similarity': SellerSimularityScores,\n",
    "                 'RFMCol':'Seller_OverallRFMScore'}\n",
    "CategoryGroups['SellerGroupId']=SellerCatDict\n",
    "\n",
    "#Add Buyer-Category Groups\n",
    "CustomerCategoryGroups = pd.read_csv(\"./BuyerCategoryUniqueGroups.csv\").sort_values(by='CustomerGroupId')\n",
    "CustomerSimularityScores = pd.read_csv(\"./BuyerCategoryCosineSimularity.csv\").sort_values(by='CustomerGroupId')\n",
    "CustomerCatDict = {'Groups':CustomerCategoryGroups, \n",
    "                 'DropCols':['size'],\n",
    "                 'CategoryColName':'product_category_name_english',\n",
    "                 'CatIdentifier':'customer_unique_id',\n",
    "                 'Similarity': CustomerSimularityScores,\n",
    "                 'RFMCol':'Buyer_OverallRFMScore'}\n",
    "CategoryGroups['CustomerGroupId']=CustomerCatDict\n",
    "\n",
    "#Add Customer-Product Groups\n",
    "CustomerProductGroups = pd.read_csv(\"./Product_Customer_UniqueGroups.csv\").sort_values(by='CustomerProductGroupId')\n",
    "CustomerProductSimularityScores = pd.read_csv(\"./Product_Customer_CosineSimularity.csv\").sort_values(by='CustomerProductGroupId')\n",
    "ProductSimDict = {'Groups':CustomerProductGroups, \n",
    "                    'DropCols':['size'],\n",
    "                    'CategoryColName':'product_id',\n",
    "                    'CatIdentifier':'customer_unique_id',\n",
    "                    'Similarity': CustomerProductSimularityScores,\n",
    "                    'RFMCol':'Buyer_OverallRFMScore'}\n",
    "CategoryGroups['CustomerProductGroupId']=ProductSimDict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генерація рекомендацій"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Знаходження спільних кластерів айдішок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OL_Data_Simp.customer_unique_id.value_counts().head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Покупки для випадкового кастомера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleCustomer = '3e43e6105506432c953e165fb2acf44c'\n",
    "OL_Data_Simp[OL_Data_Simp.customer_unique_id == exampleCustomer].product_category_name_english.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Рекомендація"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Product Ids with Category\n",
    "prodCategories = OL_Data_Simp[['product_id','product_category_name_english']].drop_duplicates()\n",
    "\n",
    "#Standard\n",
    "recs = getRecommendataionsForCustomer(OL_Data_Simp, exampleCustomer, CategoryGroups, 0.8)\n",
    "recs = recs.sort_values(by='Total', ascending=False)\n",
    "recs = recs.merge(prodCategories, how='left', on='product_id')\n",
    "\n",
    "#With RFM Consideration\n",
    "recs2 = getRecommendataionsForCustomer(OL_Data_Simp, exampleCustomer, CategoryGroups, 0.8, True)\n",
    "recs2 = recs2.sort_values(by='Total', ascending=False)\n",
    "recs2 = recs2.merge(prodCategories, how='left', on='product_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recsSub = recs.head(200)\n",
    "recsSub.loc[:,recsSub.any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs.head(200).product_category_name_english.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs2.head(200).product_category_name_english.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оновлення рекомендації"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getRecommendataionsFromCategoryGroupDict\n",
    "#  Take a Category DataFrame (columns represent categories of the Orders, and each row is a Group that sums \n",
    "#  to 100% with this percentage spread out among the columns and every row is unique) and a simularity matrix\n",
    "#  between the Groups of the Category DataFrame, and update them based on new orders \n",
    "#\n",
    "#  This function will update the categories on a customer by customer basis, needing the column to identify\n",
    "#  unique customers and the old orders the original Category DataFrame and Simularity Matrix was made from\n",
    "#  to update them.  This is not the most effiecient method, as customers perchase percentages are most likely \n",
    "#  repeated, but it does the job.  Improving the performance is left for future work\n",
    "#\n",
    "#  categoryGroupsDF - Category DataFrame as described above, columns are categories, rows are Groups\n",
    "#  categoryGroupsDF_DropCols - Columns in categoryGroupsDF to ignore\n",
    "#  categoryGroupsDF_CatColumnName - the column name in the orders that contains the categories in the columns\n",
    "#                                   of categoryGroupsDF\n",
    "#  categoryGroupsDF_groupIdName - the column name of the unique Group (row) identifier in categoryGroupsDF\n",
    "#  simularityScores - Matrix representing the simularity between every Group in the Category Groups DF Rows\n",
    "#  uniqueCustIdCol - name of the column that contains the unique customer identifier\n",
    "#  oldOrders - Orders the original Category Groups DataFrame and Simularity Matrix were made from\n",
    "#  newOrders - Orders to add/modify the Category Groups DataFrame and Simularity Matrix\n",
    "def UpdateCategoryGroupsForNewSales(categoryGroupsDF,\n",
    "                                    categoryGroupsDF_CatColumnName,\n",
    "                                    categoryGroupsDF_DropCols,\n",
    "                                    categoryGroupsDF_groupIdName,\n",
    "                                    \n",
    "                                    simularityScores,\n",
    "                                    uniqueCustIdCol,\n",
    "                                    oldOrders,\n",
    "                                    newOrders):\n",
    "    \n",
    "    #Gather all orders\n",
    "    allOrders = pd.concat([oldOrders, newOrders])\n",
    "    #Boolean to represent whether new Groups have been made\n",
    "    newGroups = False\n",
    "    \n",
    "    #Parameters used for Tracking Progress\n",
    "    ind = 0\n",
    "    lengthOfNewUsers = len(newOrders[uniqueCustIdCol].unique())\n",
    "    print(\"Updating orders of {0} users\".format(lengthOfNewUsers))\n",
    "    \n",
    "    #Update Data Structures on a user by user basis\n",
    "    for custId in newOrders[uniqueCustIdCol].unique():\n",
    "        #Print update\n",
    "        if(lengthOfNewUsers>100 and ind%(lengthOfNewUsers//10) ==0):\n",
    "                print(\"{0}% \".format(ind/lengthOfNewUsers*100))\n",
    "        ind= ind+1\n",
    "        \n",
    "        #Get old Category Group\n",
    "        oldCategory = oldOrders[oldOrders[uniqueCustIdCol]==custId][categoryGroupsDF_groupIdName]\n",
    "        \n",
    "        #Index Category by Group Id\n",
    "        catGroupsDF_Indexed = categoryGroupsDF.set_index(categoryGroupsDF_groupIdName).sort_values(by=categoryGroupsDF_groupIdName)\n",
    "        if(len(oldCategory) != 0):\n",
    "            #If Old Category exists, decrease count by one\n",
    "            #This will be replaced if new orders do not alter the percetage distribution of orders\n",
    "            oldCategory = oldCategory[0]\n",
    "            catGroupsDF_Indexed.loc[oldCategory,'size'] = catGroupsDF_Indexed.loc[oldCategory,'size'] - 1\n",
    "\n",
    "        #Remove ignored columns from CategoryGroupsDF\n",
    "        catGroupsDF_Dropped = catGroupsDF_Indexed.drop(columns=categoryGroupsDF_DropCols)\n",
    "\n",
    "        #Get all categories in the Group\n",
    "        all_categories = list(catGroupsDF_Dropped.columns)\n",
    "        all_categories = [str(x) for x in all_categories]        \n",
    "\n",
    "        #Make Customer Row From Old Orders\n",
    "        thiscustOldSubset = oldOrders[oldOrders[uniqueCustIdCol] == custId]\n",
    "        thisCustOld = np.array([len(thiscustOldSubset[thiscustOldSubset[categoryGroupsDF_CatColumnName] == x]) for x in all_categories])\n",
    "        #And Customer Row From New Orders\n",
    "        thiscustNewSubset = newOrders[newOrders[uniqueCustIdCol] == custId]\n",
    "        thisCustNew = np.array([len(thiscustNewSubset[thiscustNewSubset[categoryGroupsDF_CatColumnName] == x]) for x in all_categories])\n",
    "        \n",
    "        #Normalize Customer Row to 100%\n",
    "        custTotal = thisCustOld+thisCustNew\n",
    "        custTotalNorm = custTotal/sum(custTotal)*100/5.0\n",
    "        custTotalNorm = np.floor(custTotalNorm)\n",
    "        custTotalNorm = custTotalNorm * 5.0\n",
    "        custTotalNorm.shape=(1,len(custTotalNorm))\n",
    "        \n",
    "        #Create Customer DataFrame\n",
    "        custTotalNormDF = pd.DataFrame(data=custTotalNorm, columns=all_categories)    \n",
    "        #Get similarity between Customer DataFrame and Existing Groups\n",
    "        simScores = np.array(cosine_similarity(catGroupsDF_Dropped,custTotalNormDF))\n",
    "        \n",
    "        if(max(simScores)>=.9999):\n",
    "            #Simularity is near enough to an existing Group\n",
    "            \n",
    "            #Create and Sort Simularity Scores\n",
    "            simDF = pd.DataFrame(simScores, columns=['Simularity'])\n",
    "            simDF[categoryGroupsDF_groupIdName] = catGroupsDF_Dropped.reset_index()[categoryGroupsDF_groupIdName]\n",
    "            simDF = simDF[simDF.Simularity == max(simDF.Simularity)]\n",
    "            \n",
    "            #Find New Category for this Customer, and add 1 to the size of that group\n",
    "            newCat = simDF.head(1)[categoryGroupsDF_groupIdName]\n",
    "            catGroupsDF_Indexed.loc[newCat,'size'] = catGroupsDF_Indexed.loc[newCat,'size'] + 1\n",
    "            \n",
    "            #Reset CategroyGroup Index and set all the customer's orders to new Category\n",
    "            categoryGroupsDF = catGroupsDF_Indexed.reset_index()\n",
    "            allOrders.loc[allOrders[uniqueCustIdCol]==custId,categoryGroupsDF_groupIdName] = newCat\n",
    "        else:\n",
    "            #Matching Group Does not Exist, Create a new one\n",
    "            newGroups = True\n",
    "            #New Category Numer\n",
    "            newCat = max(categoryGroupsDF[categoryGroupsDF_groupIdName])+1\n",
    "            \n",
    "            #Add Category Number and size to Customer Row DF\n",
    "            custTotalNormDF[categoryGroupsDF_groupIdName] = newCat\n",
    "            custTotalNormDF['size'] = 1\n",
    "            #Add to Category Groups\n",
    "            categoryGroupsDF = pd.concat([categoryGroupsDF,custTotalNormDF])\n",
    "            \n",
    "            #Add simularity Scores to Simularity Matrix\n",
    "            simCols = simularityScores.set_index(categoryGroupsDF_groupIdName).columns\n",
    "            length = len(simScores)\n",
    "            simScores.shape = (1, length)\n",
    "            \n",
    "            #New Group Simularity DF Row\n",
    "            custSimScoresDF = pd.DataFrame(data=simScores, columns=simCols)\n",
    "            custSimScoresDF[categoryGroupsDF_groupIdName] = newCat\n",
    "            custSimScoresDF[str(newCat)] = 1.0\n",
    "            \n",
    "            #Add new column for new scores\n",
    "            simScores.shape = (length,1)\n",
    "            simularityScores[str(newCat)] = simScores\n",
    "            #Add New Group Row to Sim Scores\n",
    "            simularityScores = pd.concat([simularityScores,custSimScoresDF])\n",
    "            \n",
    "            #Update all customer orders\n",
    "            allOrders.loc[allOrders[uniqueCustIdCol]==custId,categoryGroupsDF_groupIdName] = newCat\n",
    "        \n",
    "    \n",
    "    if(all(categoryGroupsDF['size'] != 0) and not newGroups):\n",
    "        #If there are no groups added or removed, no need to go any further\n",
    "        return (allOrders, categoryGroupsDF, simularityScores)\n",
    "    \n",
    "    #Set Indexes\n",
    "    catGroupsDF_Indexed = categoryGroupsDF.set_index(categoryGroupsDF_groupIdName)\n",
    "    simScore_Indexed = simularityScores.set_index(categoryGroupsDF_groupIdName)\n",
    "    \n",
    "    if(any(categoryGroupsDF['size'] <= 0)):\n",
    "        #If some groups are now empty, remove them from Category Groups and Simularity Scores\n",
    "        groupsToDrop = categoryGroupsDF[categoryGroupsDF.size == 0][categoryGroupsDF_groupIdName]\n",
    "    \n",
    "        for group in groupsToDrop:\n",
    "            catGroupsDF_Indexed = catGroupsDF_Indexed.drop(group)\n",
    "            simScore_Indexed = simScore_Indexed.drop(group)\n",
    "            simScore_Indexed = simScore_Indexed.drop(columns=group)\n",
    "    \n",
    "    #Reset Indexes\n",
    "    categoryGroupsDF = catGroupsDF_Indexed.reset_index()\n",
    "    simularityScores = simScore_Indexed.reset_index()\n",
    "    \n",
    "    #Renumber all Groups, so that they are of a continuous range\n",
    "    #creating dictionary from old group number to new\n",
    "    groupsIndOld = categoryGroupsDF[categoryGroupsDF_groupIdName]\n",
    "    groupsIndNew = list(range(len(groupsIndOld)))\n",
    "    changeDict = {groupsIndOld[i] : groupsIndNew[i] for i in range(len(groupsIndOld))}\n",
    "    \n",
    "    #Update Orders to new Group Numbers\n",
    "    allOrders.replace({categoryGroupsDF_groupIdName:changeDict})\n",
    "    #Update Category Groups to new Group Numbers\n",
    "    categoryGroupsDF[categoryGroupsDF_groupIdName] = groupsIndNew\n",
    "    categoryGroupsDF = categoryGroupsDF.rename(columns=changeDict)\n",
    "    #Update Simularity Scores to new Group Numbers\n",
    "    simularityScores[categoryGroupsDF_groupIdName] = groupsIndNew\n",
    "    simularityScores = simularityScores.rename(columns=changeDict)\n",
    "    \n",
    "    return (allOrders, categoryGroupsDF, simularityScores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Приклад використання"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Subset of groups, representing the first 5 Customer Groups\n",
    "custGroups = range(0,5)\n",
    "CustomerCategoryGroupsSub = CustomerCategoryGroups[CustomerCategoryGroups.CustomerGroupId.isin(custGroups)]\n",
    "#Subset Of Simularity Scores\n",
    "CustomerSimularityScoresSub = CustomerSimularityScores[CustomerSimularityScores.CustomerGroupId.isin(custGroups)]\n",
    "columnsSubset = ['CustomerGroupId'] + [str(x) for x in custGroups]\n",
    "CustomerSimularityScoresSub = CustomerSimularityScoresSub[columnsSubset]\n",
    "#Subset of Orders\n",
    "OrdersSub = OL_Data_Simp[OL_Data_Simp.CustomerGroupId.isin(custGroups)]\n",
    "\n",
    "#New Orders Subset\n",
    "newCustGrous = range(100,105)\n",
    "newOrders = OL_Data_Simp[OL_Data_Simp.CustomerGroupId.isin(newCustGrous)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Old Category Groups\n",
    "CustomerCategoryGroupsSub.loc[:, CustomerCategoryGroupsSub.any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Old Simularity Scores\n",
    "CustomerSimularityScoresSub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CategoryGroup = CategoryGroups['CustomerGroupId']\n",
    "(orders, cat, sim) = UpdateCategoryGroupsForNewSales(CustomerCategoryGroupsSub,\n",
    "                                                        CategoryGroup['CategoryColName'],\n",
    "                                                        CategoryGroup['DropCols'],\n",
    "                                                        'CustomerGroupId',\n",
    "                                                        CustomerSimularityScoresSub,\n",
    "                                                        'customer_unique_id',\n",
    "                                                        OrdersSub,\n",
    "                                                        newOrders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New Categories\n",
    "cat.loc[:,cat.any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New Simularity\n",
    "sim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "64d6f8b7a9789d813848f18983fa6bddacf295d4ac88654b739741694ba09773"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
